{
    "abstract": "Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of polysemantic neurons, the linear representation hypothesis, modular features, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methods in the common language of causal abstraction, namely, activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and steering.",
    "authors": [
        "Atticus Geiger",
        "Duligur Ibeling",
        "Amir Zur",
        "Maheep Chaudhary",
        "Sonakshi Chauhan",
        "Jing Huang",
        "Aryaman Arora",
        "Zhengxuan Wu",
        "Noah Goodman",
        "Christopher Potts",
        "Thomas Icard"
    ],
    "emails": [
        "atticusg@gmail.com",
        "duligur@icloud.com",
        "amir.zur1212@gmail.com",
        "maheepchaudhary.research@gmail.com",
        "sonakshichauhan1402@gmail.com",
        "hij@stanford.edu",
        "aryamana@stanford.edu",
        "wuzhengx@stanford.edu",
        "ngoodman@stanford.edu",
        "cgpotts@stanford.edu",
        "icard@stanford.edu"
    ],
    "id": "23-0058",
    "issue": 83,
    "pages": [
        1,
        64
    ],
    "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
    "volume": 26,
    "year": 2025
}