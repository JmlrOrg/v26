{
    "abstract": "An important feature of kernel mean embeddings (KME) is that the rate of convergence of the empirical KME to the true distribution KME can be bounded independently of the dimension of the space, properties of the distribution and smoothness features of the kernel. We show how to speed-up convergence by leveraging variance information in the reproducing kernel Hilbert space. Furthermore, we show that even when such information is a priori unknown, we can efficiently estimate it from the data, recovering the desiderata of a distribution agnostic bound that enjoys acceleration in fortuitous settings. We further extend our results from independent data to stationary mixing sequences and illustrate our methods in the context of hypothesis testing and robust parametric estimation.",
    "authors": [
        "Geoffrey Wolfer",
        "Pierre Alquier"
    ],
    "emails": [
        "geo.wolfer@aoni.waseda.jp",
        "alquier@essec.edu"
    ],
    "id": "23-0161",
    "issue": 57,
    "pages": [
        1,
        48
    ],
    "title": "Variance-Aware Estimation of Kernel Mean Embedding",
    "volume": 26,
    "year": 2025
}