{
    "abstract": "Prior work has established Test-Time Training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is first trained on the same instance using a self-supervised task such as reconstruction. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The improvements are more than 2.2x and 1.5x for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses strictly more information, training on all frames from the entire test video regardless of temporal order. This finding challenges those in prior work using synthetic videos. We formalize a notion of locality as the advantage of online over offline TTT, and analyze its role with ablations and a theory based on bias-variance trade-off.",
    "authors": [
        "Renhao Wang",
        "Yu Sun",
        "Arnuv Tandon",
        "Yossi Gandelsman",
        "Xinlei Chen",
        "Alexei A. Efros",
        "Xiaolong Wang"
    ],
    "emails": [
        "renwang435@gmail.com",
        "yusun@berkeley.edu",
        "arnuv@stanford.edu",
        "yossi@gandelsman.com",
        "xinleic@meta.com",
        "efros@eecs.berkeley.edu",
        "xiw012@ucsd.edu"
    ],
    "extra_links": [
        [
            "code",
            "https://test-time-training.github.io/video"
        ]
    ],
    "id": "24-0439",
    "issue": 9,
    "pages": [
        1,
        29
    ],
    "title": "Test-Time Training on Video Streams",
    "volume": 26,
    "year": 2025
}